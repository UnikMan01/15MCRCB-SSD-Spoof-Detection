{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f430e3-22ac-48cf-aa9c-82a341f071b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Classes: 30\n",
      "['Barishal', 'Barishal_monster', 'Chapai', 'Chapai_monster', 'Chittagong', 'Chittagong_monster', 'Habiganj', 'Habiganj_monster', 'Kustia', 'Kustia_monster', 'Naoga', 'Naoga_monster', 'Narail', 'Narail_monster', 'Narsingdi', 'Narsingdi_monster', 'Rajshahi', 'Rajshahi_monster', 'Rangpur', 'Rangpur_monster', 'Sandwip', 'Sandwip_monster', 'Sylhet', 'Sylhet_monster', 'Tangail', 'Tangail_monster', 'kishoreganj', 'kishoreganj_monster', 'pabna', 'pabna_monster']\n",
      "train set: 27944 files\n",
      "val set: 6016 files\n",
      "test set: 5980 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AASISTLite(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (temporal): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AttentivePool(\n",
      "    (attn): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [28:28<00:00,  1.02it/s]\n",
      "Epoch 1/30 [val]: 100%|██████████████████████████████████████████████████████████████| 376/376 [04:45<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss 1.3559 Acc 0.5096 | Val Loss 0.8586 Acc 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [26:50<00:00,  1.08it/s]\n",
      "Epoch 2/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 150.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Train Loss 0.7983 Acc 0.6857 | Val Loss 0.6740 Acc 0.7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [21:49<00:00,  1.33it/s]\n",
      "Epoch 3/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 152.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Train Loss 0.6130 Acc 0.7530 | Val Loss 0.5375 Acc 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [20:35<00:00,  1.41it/s]\n",
      "Epoch 4/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 155.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Train Loss 0.4996 Acc 0.7920 | Val Loss 0.4766 Acc 0.8339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [20:00<00:00,  1.46it/s]\n",
      "Epoch 5/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 154.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Train Loss 0.4234 Acc 0.8228 | Val Loss 0.4186 Acc 0.8441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [20:00<00:00,  1.46it/s]\n",
      "Epoch 6/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 152.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Train Loss 0.3686 Acc 0.8439 | Val Loss 0.3630 Acc 0.8738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [23:30<00:00,  1.24it/s]\n",
      "Epoch 7/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 113.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Train Loss 0.3249 Acc 0.8615 | Val Loss 0.3509 Acc 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [49:20<00:00,  1.69s/it]\n",
      "Epoch 8/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 124.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Train Loss 0.2896 Acc 0.8771 | Val Loss 0.3005 Acc 0.8954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [train]: 100%|██████████████████████████████████████████████████████████| 1747/1747 [30:06<00:00,  1.03s/it]\n",
      "Epoch 9/30 [val]: 100%|█████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 153.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Train Loss 0.2607 Acc 0.8891 | Val Loss 0.3617 Acc 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [26:04<00:00,  1.12it/s]\n",
      "Epoch 10/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 149.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss 0.2364 Acc 0.9002 | Val Loss 0.2679 Acc 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [22:52<00:00,  1.27it/s]\n",
      "Epoch 11/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 150.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss 0.2147 Acc 0.9082 | Val Loss 0.2725 Acc 0.9132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [train]:  62%|███████████████████████████████████▎                     | 1084/1747 [13:37<09:00,  1.23it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 13/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [23:53<00:00,  1.22it/s]\n",
      "Epoch 13/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 147.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss 0.1825 Acc 0.9237 | Val Loss 0.2559 Acc 0.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [28:17<00:00,  1.03it/s]\n",
      "Epoch 14/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 154.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss 0.1695 Acc 0.9291 | Val Loss 0.2299 Acc 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [21:39<00:00,  1.34it/s]\n",
      "Epoch 15/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 156.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss 0.1596 Acc 0.9350 | Val Loss 0.2604 Acc 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [23:07<00:00,  1.26it/s]\n",
      "Epoch 16/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 142.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss 0.1480 Acc 0.9389 | Val Loss 0.2754 Acc 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [32:47<00:00,  1.13s/it]\n",
      "Epoch 17/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 137.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss 0.1438 Acc 0.9418 | Val Loss 0.2290 Acc 0.9343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [29:29<00:00,  1.01s/it]\n",
      "Epoch 18/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 147.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss 0.1337 Acc 0.9457 | Val Loss 0.2259 Acc 0.9382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [57:32<00:00,  1.98s/it]\n",
      "Epoch 19/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 107.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss 0.1208 Acc 0.9517 | Val Loss 0.2183 Acc 0.9437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [41:22<00:00,  1.42s/it]\n",
      "Epoch 20/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 109.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss 0.1166 Acc 0.9528 | Val Loss 0.2263 Acc 0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [train]:  53%|██████████████████████████████▋                           | 924/1747 [20:57<19:05,  1.39s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 21/30 [train]: 100%|██████████████████████████████████████████████████████| 1747/1747 [13:03:21<00:00, 26.90s/it]\n",
      "Epoch 21/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 154.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss 0.1108 Acc 0.9558 | Val Loss 0.2535 Acc 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [31:36<00:00,  1.09s/it]\n",
      "Epoch 22/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 113.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss 0.1105 Acc 0.9568 | Val Loss 0.2106 Acc 0.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [28:19<00:00,  1.03it/s]\n",
      "Epoch 23/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 151.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss 0.1010 Acc 0.9607 | Val Loss 0.2535 Acc 0.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [30:49<00:00,  1.06s/it]\n",
      "Epoch 24/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 143.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss 0.1021 Acc 0.9620 | Val Loss 0.2139 Acc 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [25:10<00:00,  1.16it/s]\n",
      "Epoch 25/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 150.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss 0.0888 Acc 0.9655 | Val Loss 0.2130 Acc 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [30:37<00:00,  1.05s/it]\n",
      "Epoch 26/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 149.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss 0.0925 Acc 0.9645 | Val Loss 0.2237 Acc 0.9461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [31:53<00:00,  1.10s/it]\n",
      "Epoch 27/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 142.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss 0.0859 Acc 0.9671 | Val Loss 0.2372 Acc 0.9461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [38:16<00:00,  1.31s/it]\n",
      "Epoch 28/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 107.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train Loss 0.0429 Acc 0.9828 | Val Loss 0.1851 Acc 0.9588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [47:03<00:00,  1.62s/it]\n",
      "Epoch 29/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:03<00:00, 106.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train Loss 0.0343 Acc 0.9862 | Val Loss 0.1838 Acc 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [train]:  23%|█████████████▍                                            | 406/1747 [10:34<22:02,  1.01it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 30/30 [train]: 100%|█████████████████████████████████████████████████████████| 1747/1747 [34:59<00:00,  1.20s/it]\n",
      "Epoch 30/30 [val]: 100%|████████████████████████████████████████████████████████████| 376/376 [00:02<00:00, 144.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss 0.0357 Acc 0.9863 | Val Loss 0.1873 Acc 0.9598\n",
      "Training done. Best Val Acc: 0.9597739361702128\n",
      "Saved best weights to best_aasist_lite.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_11632\\608732916.py:358: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_aasist_lite.pth\", map_location=device))\n",
      "Testing: 100%|█████████████████████████████████████████████████████████████████████| 5980/5980 [06:10<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ CLASSIFICATION REPORT ================\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Barishal     0.9362    0.9778    0.9565       135\n",
      "   Barishal_monster     0.8841    0.9037    0.8938       135\n",
      "             Chapai     0.9804    1.0000    0.9901       150\n",
      "     Chapai_monster     1.0000    0.9867    0.9933       150\n",
      "         Chittagong     0.9750    0.9873    0.9811       237\n",
      " Chittagong_monster     0.9118    0.9156    0.9137       237\n",
      "           Habiganj     0.8935    0.9557    0.9235       158\n",
      "   Habiganj_monster     0.9045    0.8987    0.9016       158\n",
      "             Kustia     1.0000    0.9933    0.9967       150\n",
      "     Kustia_monster     0.9868    1.0000    0.9934       150\n",
      "              Naoga     1.0000    1.0000    1.0000       150\n",
      "      Naoga_monster     1.0000    0.9933    0.9967       150\n",
      "             Narail     0.9879    0.9800    0.9839       250\n",
      "     Narail_monster     0.9869    0.9040    0.9436       250\n",
      "          Narsingdi     0.9725    0.9568    0.9646       185\n",
      "  Narsingdi_monster     0.9607    0.9243    0.9421       185\n",
      "           Rajshahi     0.9931    0.9931    0.9931       144\n",
      "   Rajshahi_monster     1.0000    0.9931    0.9965       144\n",
      "            Rangpur     0.9659    0.9714    0.9687       175\n",
      "    Rangpur_monster     0.8431    0.9829    0.9077       175\n",
      "            Sandwip     0.9884    0.9659    0.9770       176\n",
      "    Sandwip_monster     0.9641    0.9148    0.9388       176\n",
      "             Sylhet     0.9747    0.9467    0.9605       488\n",
      "     Sylhet_monster     0.9168    0.9488    0.9325       488\n",
      "            Tangail     0.9824    1.0000    0.9911       167\n",
      "    Tangail_monster     0.9529    0.9701    0.9614       167\n",
      "        kishoreganj     0.9854    0.9783    0.9818       276\n",
      "kishoreganj_monster     0.9620    0.9167    0.9388       276\n",
      "              pabna     1.0000    0.9933    0.9966       149\n",
      "      pabna_monster     0.9933    0.9933    0.9933       149\n",
      "\n",
      "           accuracy                         0.9607      5980\n",
      "          macro avg     0.9634    0.9648    0.9637      5980\n",
      "       weighted avg     0.9618    0.9607    0.9608      5980\n",
      "\n",
      "\n",
      "Saved figures:\n",
      "confusion_matrix.png, roc_multiclass.png, pr_multiclass.png, loss_curve.png, acc_curve.png\n",
      "waveform.png, spectrogram.png, chromagram.png, pca_embeddings.png, tsne_embeddings.png\n",
      "Best model: best_aasist_lite.pth\n"
     ]
    }
   ],
   "source": [
    "# ===================== AASIST-Lite (Spectro-Temporal) FULL TRAIN+EVAL CODE =====================\n",
    "# Works with your folder structure:\n",
    "# D:\\RealVsMonster_Split\\train\\<class>\\audio.*\n",
    "# D:\\RealVsMonster_Split\\val\\<class>\\audio.*\n",
    "# D:\\RealVsMonster_Split\\test\\<class>\\audio.*\n",
    "#\n",
    "# Output:\n",
    "# - best_aasist_lite.pth\n",
    "# - confusion_matrix.png\n",
    "# - roc_multiclass.png\n",
    "# - pr_multiclass.png\n",
    "# - loss_curve.png\n",
    "# - acc_curve.png\n",
    "# - waveform.png / spectrogram.png / chromagram.png\n",
    "# - pca_embeddings.png / tsne_embeddings.png\n",
    "# - classification report printed in console\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATASET_ROOT = r\"D:\\RealVsMonster_Split\"\n",
    "SAMPLE_RATE  = 16000\n",
    "\n",
    "# Feature: 2-channel -> [LogMel, Linear-Fbank(log)]  (spectral)\n",
    "N_MELS       = 64\n",
    "N_LINFB      = 64\n",
    "N_FFT        = 1024\n",
    "HOP_LENGTH   = 256\n",
    "MAX_FRAMES   = 256\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 30\n",
    "LR           = 2e-4\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "# SpecAugment\n",
    "USE_SPECAUG      = True\n",
    "TIME_MASK_PARAM  = 24\n",
    "FREQ_MASK_PARAM  = 6\n",
    "\n",
    "# Audio safety\n",
    "MIN_AUDIO_SAMPLES = 2048\n",
    "MIN_RMS = 1e-4\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------- CLASS NAMES ----------------\n",
    "train_base = os.path.join(DATASET_ROOT, \"train\")\n",
    "CLASS_NAMES = sorted([d for d in os.listdir(train_base) if os.path.isdir(os.path.join(train_base, d))])\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(\"Classes:\", NUM_CLASSES)\n",
    "print(CLASS_NAMES)\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def pad_trunc_2d(X, max_frames):\n",
    "    # X: (F, T)\n",
    "    if X.shape[1] < max_frames:\n",
    "        pad = np.zeros((X.shape[0], max_frames - X.shape[1]), dtype=np.float32)\n",
    "        X = np.concatenate([X, pad], axis=1)\n",
    "    else:\n",
    "        X = X[:, :max_frames]\n",
    "    return X\n",
    "\n",
    "def zscore_norm(X, eps=1e-6):\n",
    "    mu = float(X.mean())\n",
    "    std = float(X.std())\n",
    "    return (X - mu) / (std + eps)\n",
    "\n",
    "def safe_load_and_trim(path):\n",
    "    y, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
    "    y, _ = librosa.effects.trim(y, top_db=30)\n",
    "\n",
    "    if y is None or len(y) < MIN_AUDIO_SAMPLES:\n",
    "        return None, sr\n",
    "\n",
    "    rms = float(np.sqrt(np.mean(y**2) + 1e-12))\n",
    "    if rms < MIN_RMS:\n",
    "        return None, sr\n",
    "\n",
    "    return y, sr\n",
    "\n",
    "# ---------------- FEATURE EXTRACTION ----------------\n",
    "def extract_logmel(y, sr):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=2.0\n",
    "    )\n",
    "    mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel = zscore_norm(mel).astype(np.float32)\n",
    "    mel = pad_trunc_2d(mel, MAX_FRAMES)\n",
    "    return mel  # (64, T)\n",
    "\n",
    "def extract_log_linear_fbank(y, sr):\n",
    "    \"\"\"\n",
    "    Manual linear-frequency filterbank energies (log)  (no librosa.filters.linear dependency)\n",
    "    \"\"\"\n",
    "    S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))**2  # (F, T)\n",
    "    F_bins = S.shape[0]\n",
    "\n",
    "    freqs = np.linspace(0, sr/2, F_bins, dtype=np.float32)\n",
    "    edges = np.linspace(0, sr/2, N_LINFB + 2, dtype=np.float32)\n",
    "\n",
    "    fb = np.zeros((N_LINFB, F_bins), dtype=np.float32)\n",
    "    for m in range(N_LINFB):\n",
    "        f_left, f_center, f_right = edges[m], edges[m+1], edges[m+2]\n",
    "        left  = (freqs - f_left) / (f_center - f_left + 1e-9)\n",
    "        right = (f_right - freqs) / (f_right - f_center + 1e-9)\n",
    "        fb[m] = np.maximum(0.0, np.minimum(left, right))\n",
    "\n",
    "    E = np.dot(fb, S) + 1e-8\n",
    "    E = np.log(E)\n",
    "\n",
    "    E = zscore_norm(E).astype(np.float32)\n",
    "    E = pad_trunc_2d(E, MAX_FRAMES)\n",
    "    return E  # (64, T)\n",
    "\n",
    "def extract_features(path):\n",
    "    y, sr = safe_load_and_trim(path)\n",
    "    if y is None:\n",
    "        mel = np.zeros((N_MELS, MAX_FRAMES), dtype=np.float32)\n",
    "        lfb = np.zeros((N_LINFB, MAX_FRAMES), dtype=np.float32)\n",
    "    else:\n",
    "        mel = extract_logmel(y, sr)\n",
    "        lfb = extract_log_linear_fbank(y, sr)\n",
    "    X = np.stack([mel, lfb], axis=0).astype(np.float32)  # (2, 64, T)\n",
    "    return X\n",
    "\n",
    "# ---------------- SPECAUGMENT ----------------\n",
    "def spec_augment(x, time_mask_param=24, freq_mask_param=6):\n",
    "    # x: torch tensor (C,F,T) ; masks apply all channels\n",
    "    if not USE_SPECAUG:\n",
    "        return x\n",
    "    C, F, T = x.shape\n",
    "\n",
    "    f = random.randint(0, min(freq_mask_param, F))\n",
    "    f0 = random.randint(0, max(0, F - f))\n",
    "    if f > 0:\n",
    "        x[:, f0:f0+f, :] = 0\n",
    "\n",
    "    t = random.randint(0, min(time_mask_param, T))\n",
    "    t0 = random.randint(0, max(0, T - t))\n",
    "    if t > 0:\n",
    "        x[:, :, t0:t0+t] = 0\n",
    "\n",
    "    return x\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class SpecDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        base = os.path.join(root, split)\n",
    "        self.split = split\n",
    "        self.paths, self.labels = [], []\n",
    "        self.cls_to_idx = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
    "\n",
    "        for cls in CLASS_NAMES:\n",
    "            folder = os.path.join(base, cls)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for f in os.listdir(folder):\n",
    "                if f.lower().endswith((\".mp3\", \".wav\", \".ogg\", \".flac\", \".m4a\")):\n",
    "                    self.paths.append(os.path.join(folder, f))\n",
    "                    self.labels.append(self.cls_to_idx[cls])\n",
    "\n",
    "        print(f\"{split} set: {len(self.paths)} files\")\n",
    "        self.cache = {}  # cache val/test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.split != \"train\" and path in self.cache:\n",
    "            feat = self.cache[path]\n",
    "        else:\n",
    "            feat = extract_features(path)\n",
    "            if self.split != \"train\":\n",
    "                self.cache[path] = feat\n",
    "\n",
    "        x = torch.tensor(feat)  # (2,64,T)\n",
    "        if self.split == \"train\":\n",
    "            x = spec_augment(x, TIME_MASK_PARAM, FREQ_MASK_PARAM)\n",
    "\n",
    "        return x, torch.tensor(label, dtype=torch.long), path\n",
    "\n",
    "train_ds = SpecDataset(DATASET_ROOT, \"train\")\n",
    "val_ds   = SpecDataset(DATASET_ROOT, \"val\")\n",
    "test_ds  = SpecDataset(DATASET_ROOT, \"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1,          shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# ---------------- MODEL (AASIST-Lite): CNN(spectral) + Transformer(temporal) + AttnPool ----------------\n",
    "class AttentivePool(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(d, d//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d//2, 1)\n",
    "        )\n",
    "    def forward(self, x):   # (B,T,D)\n",
    "        w = torch.softmax(self.attn(x), dim=1)  # (B,T,1)\n",
    "        return (w * x).sum(dim=1)               # (B,D)\n",
    "\n",
    "class AASISTLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (B, 2, 64, T)\n",
    "    CNN -> spectral patterns\n",
    "    Transformer -> temporal patterns\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, d_model=256, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),  # F/2, T/2\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),  # F/4, T/4\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),  # F/8, T/8\n",
    "        )\n",
    "\n",
    "        f_after = N_MELS // 8  # 64 -> 8\n",
    "        self.proj = nn.Linear(128 * f_after, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n",
    "        )\n",
    "        self.temporal = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.pool = AttentivePool(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)                      # (B,128,F',T')\n",
    "        B, C, F, T = z.shape\n",
    "        z = z.permute(0, 3, 1, 2).contiguous()  # (B,T,C,F)\n",
    "        z = z.view(B, T, C*F)                   # (B,T,128*F')\n",
    "        z = self.proj(z)                        # (B,T,d_model)\n",
    "\n",
    "        z = self.temporal(z)                    # (B,T,d_model)  (temporal features)\n",
    "        emb = self.pool(z)                      # (B,d_model)\n",
    "        logits = self.head(emb)                 # (B,num_classes)\n",
    "        return logits, emb\n",
    "\n",
    "model = AASISTLite(num_classes=NUM_CLASSES).to(device)\n",
    "print(model)\n",
    "\n",
    "# ---------------- LOSS/OPT ----------------\n",
    "counts = np.bincount(train_ds.labels, minlength=NUM_CLASSES).astype(np.float32)\n",
    "w = (counts.sum() / (counts + 1e-6))\n",
    "w = w / w.mean()\n",
    "class_weights = torch.tensor(w, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "def acc_from_logits(logits, y):\n",
    "    return (torch.argmax(logits, 1) == y).float().mean().item()\n",
    "\n",
    "# ---------------- TRAIN ----------------\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "best_val = -1.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    tr_loss_sum, tr_acc_sum, tr_n = 0.0, 0.0, 0\n",
    "\n",
    "    for x, y, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\"):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        tr_loss_sum += loss.item() * bs\n",
    "        tr_acc_sum  += acc_from_logits(logits.detach(), y) * bs\n",
    "        tr_n += bs\n",
    "\n",
    "    train_loss = tr_loss_sum / tr_n\n",
    "    train_acc  = tr_acc_sum / tr_n\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    va_loss_sum, va_acc_sum, va_n = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [val]\"):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            bs = y.size(0)\n",
    "            va_loss_sum += loss.item() * bs\n",
    "            va_acc_sum  += acc_from_logits(logits, y) * bs\n",
    "            va_n += bs\n",
    "\n",
    "    val_loss = va_loss_sum / va_n\n",
    "    val_acc  = va_acc_sum / va_n\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss {train_loss:.4f} Acc {train_acc:.4f} | Val Loss {val_loss:.4f} Acc {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val + 1e-6:\n",
    "        best_val = val_acc\n",
    "        torch.save(model.state_dict(), \"best_aasist_lite.pth\")\n",
    "\n",
    "print(\"Training done. Best Val Acc:\", best_val)\n",
    "print(\"Saved best weights to best_aasist_lite.pth\")\n",
    "\n",
    "# ---------------- TEST + REPORTS + CURVES ----------------\n",
    "model.load_state_dict(torch.load(\"best_aasist_lite.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "probs_all = []\n",
    "embs, emb_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, _ in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x = x.to(device)\n",
    "        logits, emb = model(x)\n",
    "        prob = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred = int(np.argmax(prob))\n",
    "\n",
    "        y_true.append(int(y.item()))\n",
    "        y_pred.append(pred)\n",
    "        probs_all.append(prob)\n",
    "        embs.append(emb.cpu().numpy()[0])\n",
    "        emb_labels.append(int(y.item()))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "probs_all = np.array(probs_all)\n",
    "embs = np.array(embs)\n",
    "emb_labels = np.array(emb_labels)\n",
    "\n",
    "print(\"\\n================ CLASSIFICATION REPORT ================\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix (AASIST-Lite)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ROC + PR (multi-class OVR)\n",
    "Y_bin = label_binarize(y_true, classes=list(range(NUM_CLASSES)))\n",
    "\n",
    "# ROC\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(Y_bin[:, i], probs_all[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{CLASS_NAMES[i]} (AUC={roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.title(\"Multi-class ROC Curve (AASIST-Lite)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(fontsize=7, loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"roc_multiclass.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# PR\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(NUM_CLASSES):\n",
    "    prec, rec, _ = precision_recall_curve(Y_bin[:, i], probs_all[:, i])\n",
    "    pr_auc = auc(rec, prec)\n",
    "    plt.plot(rec, prec, label=f\"{CLASS_NAMES[i]} (AUC={pr_auc:.2f})\")\n",
    "plt.title(\"Multi-class Precision-Recall Curve (AASIST-Lite)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(fontsize=7, loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pr_multiclass.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Train/Val curves\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Training vs Validation Loss (AASIST-Lite)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.title(\"Training vs Validation Accuracy (AASIST-Lite)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acc_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Waveform + Spectrogram + Chromagram (one test file)\n",
    "sample_path = test_ds.paths[0] if len(test_ds.paths) else None\n",
    "if sample_path:\n",
    "    y, sr = librosa.load(sample_path, sr=SAMPLE_RATE, mono=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(np.linspace(0, len(y)/sr, len(y)), y)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"waveform.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH)), ref=np.max)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(D, sr=sr, hop_length=HOP_LENGTH, x_axis=\"time\", y_axis=\"hz\")\n",
    "    plt.colorbar(format=\"%+0.0f dB\")\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"spectrogram.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, tuning=0.0)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    librosa.display.specshow(chroma, sr=sr, hop_length=HOP_LENGTH, x_axis=\"time\", y_axis=\"chroma\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Chromagram\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"chromagram.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# PCA / t-SNE embeddings\n",
    "pca = PCA(n_components=2, random_state=RANDOM_SEED)\n",
    "Zp = pca.fit_transform(embs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(Zp[:, 0], Zp[:, 1], c=emb_labels, s=10)\n",
    "plt.title(\"PCA of AASIST-Lite Embeddings\")\n",
    "plt.colorbar(sc)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pca_embeddings.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=RANDOM_SEED, init=\"pca\", learning_rate=\"auto\")\n",
    "Zt = tsne.fit_transform(embs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(Zt[:, 0], Zt[:, 1], c=emb_labels, s=10)\n",
    "plt.title(\"t-SNE of AASIST-Lite Embeddings\")\n",
    "plt.colorbar(sc)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tsne_embeddings.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nSaved figures:\")\n",
    "print(\"confusion_matrix.png, roc_multiclass.png, pr_multiclass.png, loss_curve.png, acc_curve.png\")\n",
    "print(\"waveform.png, spectrogram.png, chromagram.png, pca_embeddings.png, tsne_embeddings.png\")\n",
    "print(\"Best model: best_aasist_lite.pth\")\n",
    "# ==============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997055f3-ce6c-4fbf-bf07-b6707fbf2f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU PyTorch)",
   "language": "python",
   "name": "gpu-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
